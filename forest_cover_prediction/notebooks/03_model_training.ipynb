{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb2cf66f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab51b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"../data/interim/train_split.csv\")\n",
    "val_df   = pd.read_csv(\"../data/interim/validation_split.csv\")\n",
    "\n",
    "target_col = \"Cover_Type\"\n",
    "\n",
    "# Separate features & target\n",
    "X_train_scaled = train_df.drop(target_col, axis=1)\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_val_scaled = val_df.drop(target_col, axis=1)\n",
    "y_val   = val_df[target_col]\n",
    "\n",
    "\n",
    "# ---- Logistic Regression (Multiclass) ----\n",
    "log_reg = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "val_pred_log = log_reg.predict(X_val_scaled)\n",
    "val_acc_log  = accuracy_score(y_val, val_pred_log)\n",
    "print(f\"Logistic Regression Validation Accuracy: {val_acc_log:.4f}\")\n",
    "\n",
    "joblib.dump(log_reg, \"../models/logreg_multiclass.joblib\")\n",
    "print(\"Saved Logistic Regression model as logreg_multiclass.joblib\")\n",
    "\n",
    "#0.7238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecb1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b87349",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9970cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_rbf = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    C=1.0,\n",
    "    gamma=\"scale\",\n",
    "    decision_function_shape=\"ovr\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "val_pred_svm = svm_rbf.predict(X_val_scaled)\n",
    "val_acc_svm  = accuracy_score(y_val, val_pred_svm)\n",
    "print(f\"SVM (RBF) Validation Accuracy: {val_acc_svm:.4f}\")\n",
    "\n",
    "joblib.dump(svm_rbf, \"../models/svm_rbf_multiclass.joblib\")\n",
    "#0.8306\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879510b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93a956f6",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c18bd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Validation Accuracy: 0.8763\n",
      "Saved Neural Network model as mlp_multiclass.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn_model.fit(X_train_scaled, y_train)\n",
    "val_pred_nn = nn_model.predict(X_val_scaled)\n",
    "val_acc_nn  = accuracy_score(y_val, val_pred_nn)\n",
    "print(f\"Neural Network Validation Accuracy: {val_acc_nn:.4f}\")\n",
    "\n",
    "joblib.dump(nn_model, \"../models/mlp_multiclass.joblib\")\n",
    "print(\"Saved Neural Network model as mlp_multiclass.joblib\")\n",
    "#0.8763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355f8b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 17:11:06,536] A new study created in memory with name: no-name-4338f6aa-cc5f-4017-a97c-3e1702b4ab35\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (200, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_330347/3848924978.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform(\"learning_rate_init\", 1e-4, 1e-2)\n",
      "/tmp/ipykernel_330347/3848924978.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform(\"alpha\", 1e-5, 1e-2)\n",
      "[I 2025-12-12 18:10:22,071] Trial 0 finished with value: 0.9263585459886176 and parameters: {'hidden_layer_sizes': (200, 100), 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.0025986212005703682, 'alpha': 0.004727100327725838}. Best is trial 0 with value: 0.9263585459886176.\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/anuja/ml/smoker_status_prediction/venv/lib/python3.12/site-packages/optuna/distributions.py:502: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (200, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_330347/3848924978.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate_init = trial.suggest_loguniform(\"learning_rate_init\", 1e-4, 1e-2)\n",
      "/tmp/ipykernel_330347/3848924978.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform(\"alpha\", 1e-5, 1e-2)\n",
      "[I 2025-12-12 18:14:58,582] Trial 1 finished with value: 0.8242036901046448 and parameters: {'hidden_layer_sizes': (50,), 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.00016657330354312284, 'alpha': 0.003687938549565407}. Best is trial 0 with value: 0.9263585459886176.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 0.9263585459886176\n",
      "Best Hyperparameters: {'hidden_layer_sizes': (200, 100), 'activation': 'relu', 'solver': 'adam', 'learning_rate_init': 0.0025986212005703682, 'alpha': 0.004727100327725838}\n",
      "Saved tuned Neural Network model as mpl_tuned_optuna.joblib\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Objective: define what Optuna should optimize\n",
    "# --------------------------------------------------------------------\n",
    "def objective(trial):\n",
    "    # 1) Hyperparameter search space\n",
    "    hidden_layer_sizes = trial.suggest_categorical(\n",
    "        \"hidden_layer_sizes\",\n",
    "        [(50,), (100,), (100, 50), (150, 100, 50), (200, 100)]\n",
    "    )\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"adam\", \"sgd\"])\n",
    "    learning_rate_init = trial.suggest_loguniform(\"learning_rate_init\", 1e-4, 1e-2)\n",
    "    alpha = trial.suggest_loguniform(\"alpha\", 1e-5, 1e-2)\n",
    "\n",
    "    # 2) Create model with trial config\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        alpha=alpha,\n",
    "        max_iter=400,               # max epochs\n",
    "        random_state=42,\n",
    "        early_stopping=True,         # stop early if no improvement\n",
    "        n_iter_no_change=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 3) Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 4) Validate\n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Run Optuna Study\n",
    "# --------------------------------------------------------------------\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=30,       # adjust based on time/budget\n",
    "    timeout=3600       # seconds (optional: end after ~1 hour)\n",
    ")\n",
    "\n",
    "print(\"Best Validation Accuracy:\", study.best_value)\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Train the Best Model on Full Train + Validation Combined (Optional)\n",
    "# --------------------------------------------------------------------\n",
    "best_params = study.best_params\n",
    "\n",
    "best_nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=best_params[\"hidden_layer_sizes\"],\n",
    "    activation=best_params[\"activation\"],\n",
    "    solver=best_params[\"solver\"],\n",
    "    learning_rate_init=best_params[\"learning_rate_init\"],\n",
    "    alpha=best_params[\"alpha\"],\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20\n",
    ")\n",
    "\n",
    "# Fit final tuned model\n",
    "best_nn_model.fit(np.vstack((X_train_scaled, X_val_scaled)),\n",
    "                  np.hstack((y_train, y_val)))\n",
    "\n",
    "# Save final model\n",
    "joblib.dump(best_nn_model, \"../models/mpl_tuned_optuna.joblib\")\n",
    "\n",
    "print(\"Saved tuned Neural Network model as mpl_tuned_optuna.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_rbf = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    C=1.0,\n",
    "    gamma=\"scale\",\n",
    "    decision_function_shape=\"ovr\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "val_pred_svm = svm_rbf.predict(X_val_scaled)\n",
    "val_acc_svm  = accuracy_score(y_val, val_pred_svm)\n",
    "print(f\"SVM (RBF) Validation Accuracy: {val_acc_svm:.4f}\")\n",
    "\n",
    "joblib.dump(svm_rbf, \"../models/svm_rbf_multiclass.joblib\")\n",
    "#0.8306\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07259a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "parameters = {'C': [0.1, 1, 10], 'gamma': [0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "searcher = GridSearchCV(svm, parameters, cv=5)\n",
    "\n",
    "# Run the search on the training data\n",
    "searcher.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best CV params:\", searcher.best_params_)\n",
    "\n",
    "# Print the cross-validation accuracy\n",
    "print(\"Best CV accuracy:\", searcher.best_score_)\n",
    "\n",
    "# Evaluate accuracy on the test set\n",
    "test_accuracy = searcher.score(X_val_scaled, y_val)\n",
    "\n",
    "print(\"Test accuracy of best grid search hypers:\", test_accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
